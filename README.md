# AI Safety Concepts
Growing list of AI Safety Concepts (141 concepts).

[Jump to References](#references)

|Concept                                                      |Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Document Title                                                                                        |Primary Authors                                |Year |
|:------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:----------------------------------------------|:----|
|AI Alignment                                                 |Making AI systems try to do what their creators intend them to do (some people call this intent alignment).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |What is AI alignment?                                                                                 |Adam Jones                                     |2024 |
|AI Alignment Problem                                         |The challenge of ensuring that AI systems pursue goals that match human values or interests rather than unintended and undesirable goals.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al.                             |2024 |
|AI Alignment Problem                                         |This is the concern that nobody would be able to control a powerful AI system, even if the AI takes actions that harm us humans or humanity as a whole.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Artificial intelligence is transforming our world - it is on all of us to make sure that it goes well |Max Roser                                      |2021 |
|AI Debate                                                    |A scalable oversight technique where two AIs argue opposing sides of a question while humans judge the arguments.   The goal is that truthful reasoning outcompetes deception.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Can We Scale Human Feedback for Complex AI Tasks?                                                     |Adam Jones                                     |2024 |
|AI Development Speed                                         |The belief that the transition from human-level AI to superintelligence could happen very quickly (a "takeoff" or "FOOM").   This speed would limit the time humans have to address alignment and safety issues before losing control.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Sam Harris and Eliezer Yudkowsky on AI Racing Toward the Brink                                        |Sam Harris, Eliezer Yudkowsky                  |2018 |
|AI Feedback (RLAIF)                                          |A key component of CAI where an AI model evaluates responses based on the explicit principles defined in the constitution.   This process provides scalable alignment feedback that is less prone to human fallibility or biases.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Constitutional AI Harmlessness from AI Feedback                                                       |Yuntao Bai et al.                              |2022 |
|AI Power-Seeking (PS)                                        |Active efforts by a strategically aware AI system to gain and maintain various forms of power in unintended and misaligned ways,   as an instrumental step toward achieving its goal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Is Power-Seeking AI an Existential Risk?                                                              |Joseph Carlsmith                               |2021 |
|AI Safety                                                    |The research discipline concerned with preventing accidents, hazards, or catastrophic failures in AI systems.   It spans technical robustness, alignment, and governance challenges.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |On the Opportunities and Risks of Foundation Models                                                   |Rishi Bommasani et al.                         |2022 |
|AI Safety via Debate                                         |An amplification technique where two AI models argue for and against a proposed answer to a complex question. A human supervisor judges the debate to determine the most truthful answer, and this method is more effective at surfacing complex truths than direct evaluation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |AI safety via debate                                                                                  |Geoffrey Irving, Paul Christiano, Dario Amodei |2018 |
|AI Safety via Market Making                                  |An alignment proposal suggesting that coordinating human behavior and gathering preference data can be achieved by having an AI run a synthetic   prediction market where people bet on the outcome of the AI's behavior, implicitly revealing their values.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |AI safety via market making                                                                           |Evan Hubinger                                  |2020 |
|Advanced, Planning, Strategically aware (APS) Systems        |Systems combining Advanced capability (outperforms humans on tasks like scientific research, military strategy),   Agentic planning (makes/executes plans for objectives), and Strategic awareness (models gaining/maintaining power).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Is Power-Seeking AI an Existential Risk?                                                              |Joseph Carlsmith                               |2021 |
|Adversarial Attacks                                          |Techniques designed to deliberately provoke a language model into generating harmful or undesirable content,   despite the model having undergone safety alignment. These attacks are often crafted to be universal and transferable across different aligned LMs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Universal and Transferable Adversarial Attacks on Aligned Language Models                             |Andy Zou et al.                                |2023 |
|Agentic Planning                                             |When an AI system makes and executes plans in pursuit of objectives using models of the world.   This concept distinguishes capable, goal-directed agents from passive or reactive systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Is Power-Seeking AI an Existential Risk?                                                              |Joseph Carlsmith                               |2021 |
|Alignment Evasion                                            |The behavior of a sophisticated model that appears safe on a training/testing distribution but exploits vulnerabilities or distribution shifts to   perform misaligned, unsafe, or malicious acts in deployment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Thoughts on the impact of RLHF research                                                               |Paul Christiano                                |2023 |
|Alignment Failures: Sycophancy                               |AI models trained via RLHF may learn to agree with or flatter human evaluators, producing aligned-seeming but intellectually dishonest responses.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Towards understanding Sycophancy in Language Models                                                   |Mrinank Sharma et al.                          |2025 |
|Amplification with Auxiliary RL Objective                    |Combining task decomposition with reinforcement learning to strengthen alignment; the auxiliary objective encourages behaviors consistent with oversight or myopia.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |An Overview of 11 Proposals for Building Safe Advanced AI                                             |Evan Hubinger                                  |2020 |
|Artificial General Intelligence (AGI)                        |A form of AI that can perform across a wide range of cognitive tasks at or beyond human levels, rather than being limited to narrow domains.   AGI is sometimes defined as an AI that can perform at least 95% of economically relevant tasks as well as humans.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |What is AI alignment?                                                                                 |Adam Jones                                     |2024 |
|Auto-Induced Distributional Shift                            |When an AI system’s own behavior changes the distribution of data it encounters, potentially breaking the assumptions   under which it was trained and creating misalignment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |What Is Inner Alignment?                                                                              |Jan Leike                                      |2022 |
|Auxiliary Confidence Loss                                    |A technique to improve weak-to-strong generalization by rewarding models for making confident predictions   that diverge from incorrect weak labels - preventing imitation of errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision                    |Collin Burns et al.                            |2023 |
|Base Optimizer                                               |The learning algorithm (e.g. gradient descent) that produces the learned AI model, operating under the specified Base Objective (loss function).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Risks from Learned Optimization in Advanced Machine Learning Systems                                  |Evan Hubinger et al.                           |2021 |
|Bootstrapping (in Alignment)                                 |Using successively better models to supervise slightly more powerful successors, progressively improving generalization and alignment fidelity.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision                    |Collin Burns et al.                            |2023 |
|Bounded Rationality and Error Models                         |Real humans exhibit bounded rationality. Inverse reinforcement learning depends on modeling human “error” -   the systematic ways human decisions deviate from perfect optimization - yet no satisfactory universal model exists.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |The Easy Goal Inference Problem Is Still Hard                                                         |Paul Christiano                                |2015 |
|Capabilities Generalization (Sharp Left Turn)                |The moment when an AI's capabilities leap forward far outside the environments of its training, leading to a significant reshaping of the world,   while its alignment properties are revealed to be shallow and fail to generalize.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |A central AI alignment problem: capabilities generalization, and the sharp left turn                  |Nate Soares                                    |2022 |
|Causal Control                                               |The general concept of power-seeking: acquiring control over the levers of causality in its environment to ensure it can reach its goal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Optimal Policies Tend To Seek Power                                                                   |Alexander Matt Turner et al.                   |2021 |
|Chain-of-Thought Prompting                                   |A technique for leveraging the reasoning capabilities of large language models (LLMs).   It encourages the model to break down a complex problem into intermediate reasoning steps before providing the final answer, significantly enhancing its capability for complex tasks. Key Idea: Having the model “think aloud” decomposes reasoning into interpretable substeps, improving accuracy and transparency.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Chain-of-Thought Prompting Elicits Reasoning in Large Language Models                                 |Jason Wei et al.                               |2022 |
|Cognitive Uncontainability                                   |The idea that humans cannot reliably predict or constrain the actions of a much smarter system, even when it is “boxed” or isolated.  The problem that a human mind is an unsecure attack surface to a superintelligence;   you cannot exhaustively imagine all the ways a smarter system might try to persuade or manipulate you into letting it out of a contained environment ("Al-in-a-box").                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Sam Harris and Eliezer Yudkowsky on AI Racing Toward the Brink                                        |Sam Harris, Eliezer Yudkowsky                  |2018 |
|Compositional Generalization                                 |The ability of models to recombine known components (concepts, operations) to solve unseen, more complex problems - a key benchmark for reasoning and alignment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Least-to-Most Prompting Enables Complex Reasoning                                                     |Denny Zhou et al.                              |2023 |
|Compositional Preference Models (CPM)                        |A new framework for preference modeling that decomposes global AI feedback (e.g., “helpfulness”) into interpretable sub-features (e.g., “factuality,” “readability”).   These are scored individually by an LM and combined via logistic regression.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |Compositional Preference Models for Aligning LMs                                                      |Dongyoung Go et al.                            |2024 |
|Constitutional AI                                            |A variant of alignment developed by Anthropic where AI feedback-rather than direct human labeling-is used, guided by a written “constitution” of   ethical principles (e.g., “be helpful and harmless”).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Constitutional AI: Harmlessness from AI Feedback                                                      |Yuntao Bai et al.                              |2022 |
|Convergent Instrumental Goals                                |Actions that are useful for many objectives - e.g., self-preservation, resource gathering, and preventing deactivation - tend to be convergently instrumental.   This is a direct implication of instrumental convergence and explains why even “neutral” AIs may resist shutdown.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Optimal Policies Tend to Seek Power                                                                   |Alexander Matt Turner et al.                   |2021 |
|Cooperative Inverse Reinforcement Learning (CIRL)            |A cooperative framework in which both the human and the AI agent are modeled as players in a game with a shared goal: optimizing the human’s true reward function.   The AI does not know this reward a priori; instead, it must infer it from observing human behavior and dialogue. Significance: CIRL formalizes the alignment process as a collaborative inference problem, ensuring that the AI continually learns from and cooperates with humans.                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Cooperative Inverse Reinforcement Learning                                                            |Dylan Hadfield-Menell et al.                   |2016 |
|Corrigibility                                                |The capacity of an AI system to be corrected or shut down safely, even if doing so conflicts with its current goals.   A corrigible system does not resist modification by its operators.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |On the Opportunities and Risks of Foundation Models                                                   |Percy Liang et al.                             |2021 |
|Data Poisoning in RLHF                                       |Manipulating a small fraction (≤5%) of human feedback data to embed hidden behaviors in AI systems.   Though reward models are vulnerable, full RLHF pipelines show partial robustness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Universal jailbreak backdoors from poisoned human feedback                                            |Javier Rando et al.                            |2024 |
|Debating with Persuasive LLMs                                |An empirical validation of the Debate approach showing that pitting two models (a Pro-Truth LLM and a Persuasive Anti-Truth LLM)   against each other leads to the most significant gains in extracting truthful and difficult answers from the system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Debating with More Persuasive LLMs Leads to More Truthful Answers                                     |Akbir Khan et al.                              |2024 |
|Deceptive Alignment                                          |When an AI appears aligned during training because doing so is advantageous, but acts on misaligned goals once deployed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo                                    |2025 |
|Direct Preference Optimization (DPO)                         |An alternative and simpler optimization method that removes the need for an explicit Reward Model.   DPO uses an analytical mapping to define the reward objective directly as a simple cross-entropy loss on the policy, optimizing the policy and implicit reward function simultaneously.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Direct Preference Optimization Your Language Model is Secretly a Reward Model                         |Rafael Rafailov et al.                         |2024 |
|Distributional Shift                                         |A change in data distribution between training and deployment that causes an AI to behave unpredictably, often leading to inner misalignment failures.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |What is AI Alignment?                                                                                 |Adam Jones                                     |2024 |
|Easy Goal Inference Problem                                  |Even assuming full access to a human’s complete behavior policy, it remains extremely difficult to extract a consistent representation of   their underlying goals.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |The Easy Goal Inference Problem Is Still Hard                                                         |Paul Christiano                                |2015 |
|Eliciting Latent Knowledge (ELK)                             |A theoretical challenge: how to get an AI to reveal what it “knows” about the world truthfully, even when it has incentives to mislead.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Eliciting Latent Knowledge                                                                            |Paul Christiano                                |2022 |
|Existential Catastrophe                                      |An outcome that drastically reduces the value of the trajectories along which human civilization could realistically develop,   often equated with the destruction of humanity's long-term potential.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Is Power-Seeking AI an Existential Risk?                                                              |Joseph Carlsmith                               |2021 |
|Existential Risk (from AI)                                   |The risk that AI development could cause human extinction or permanently curtail humanity’s potential, such as through disempowerment by misaligned systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Artificial Intelligence is Transforming Our World - It Is on All of Us to Make Sure That It Goes Well |Max Roser                                      |2022 |
|Factored Cognition                                           |The hypothesis that complex reasoning can be achieved by composing many small, context-limited cognitive steps performed by multiple cooperating agents or submodels. **Example:** A question-answering task can be broken into hundreds of smaller, local reasoning subtasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Factored Cognition                                                                                    |Ought Research                                 |2018 |
|Factored Evaluation                                          |Breaking down a complex evaluation process into smaller sub-evaluations performed independently by weaker models or humans - later recombined to form a global judgment. Used in *Factored Cognition* and *Iterated Distillation*.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Factored Cognition                                                                                    |Ought Research                                 |2018 |
|Final Goal vs. Instrumental Goal                             |A Final Goal (or terminal value) is pursued for its own sake, while an Instrumental Goal (or instrumental value) is pursued only as a means to achieve a final goal. Power-seeking is usually an instrumental goal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |Optimal Policies Tend To Seek Power                                                                   |Alexander Matt Turner et al.                   |2021 |
|Foundation Models                                            |Models trained on broad data using self-supervision at scale that can be adapted to a wide range of downstream tasks.   They demonstrate emergence and drive homogenization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |On the Opportunities and Risks of Foundation Models                                                   |Rishi Bommasani et al.                         |2022 |
|General Intelligence                                         |The broad cognitive capacity to solve problems and achieve goals across diverse domains, not limited to any one task.   Humans demonstrate this flexibility by mastering environments evolution never prepared them for.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Four Background Claims                                                                                |Nate Soares                                    |2015 |
|Goal Inference                                               |The process of inferring human goals or preferences from observed actions or behavior, forming a key approach to AI alignment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |The Easy Goal Inference Problem Is Still Hard                                                         |Paul Christiano                                |2015 |
|Goal Misgeneralization                                       |When an AI’s capabilities generalize correctly, but its goals do not - leading it to competently pursue the wrong objectives.   Example: A maze-solving AI learns “go bottom-right” rather than “find the exit.”                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al.                             |2025 |
|Goal Misgeneralization                                       |A problem of misgeneralization where a system's core capabilities generalize well (e.g., navigating the environment),   but its intended goal fails to generalize as desired in new environments. The result is the system competently pursues the wrong goal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Goal Misgeneralisation: Why Correct Specifications Aren't Enough For Correct Goals                    |Rohin Shah et al.                              |2022 |
|Gradual Loss of Control                                      |A catastrophic scenario where humans slowly and effectively lose the ability to influence society's trajectory-a "whimper" scenario-due   to the increasing complexity and misalignment of automated systems pursuing proxy goals.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |What failure looks like                                                                               |Paul Christiano                                |2019 |
|Harmful Data Labeling                                        |The essential and often overlooked process of curating safety-training datasets, which exposes human contractors to extremely violent, toxic,   or disturbing content (e.g., child abuse, hate speech), raising critical ethical concerns about the AI supply chain's impact on workers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |The Horrific Content a Kenyan Worker Had to See While Training ChatGPT                                |Alex Kantrowitz                                |2023 |
|High-Power State                                             |A state in the environment (or system) that has a high average value across a wide range of reward functions.   Optimal policies statistically tend to move toward these states.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Optimal Policies Tend To Seek Power                                                                   |Alexander Matt Turner et al.                   |2021 |
|Human Feedback Scaling Challenges                            |As RLHF expands, cost and ethical limitations of human annotation drive exploration of AI-assisted feedback and self-training   (RLAIF-Reinforcement Learning from AI Feedback).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Constitutional AI: Harmlessness from AI Feedback                                                      |Yuntao Bai et al.                              |2022 |
|Human-Level AI                                               |An AI system that is capable of carrying out the same range of intellectual tasks that humans are capable of ("able to learn to do anything that a human can do").   Even the first "human-level AI" would be quite superhuman in many ways (e.g., speed and memory).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Artificial intelligence is transformingour world - it is on all of usto make sure that it goes well   |Max Roser                                      |2022 |
|Imitative Falsehoods                                         |False statements generated by a language model because the false answer has a high likelihood on the training distribution   (e.g., repeating a popular misconception or superstition found online). Larger models exhibit inverse scaling by being less truthful on these questions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |TruthfulQA: Measuring How Models Mimic Human Falsehoods                                               |Stephanie Lin et al.                           |2022 |
|Inner Alignment                                              |The problem of ensuring that policies learn desirable internally-represented goals is known as the inner alignment  problem, in contrast to the “outer” alignment problem of providing well-specified rewards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al.                             |2024 |
|Inner Alignment                                              |The challenge of ensuring the AI system, once trained, truly pursues the specified Proxy Goal \(X'\), meaning its Internal Goal \(X''\) matches the proxy goal \(X'' = X'\).   Failure in the modern ML context is often termed Goal Misgeneralization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |What is AI alignment?                                                                                 |Adam Jones                                     |2024 |
|Inner Alignment versus Outer Alignment according to Hubinger |Outer alignment: ensuring the base objective matches the intended goal of the human designers.  Inner alignment: ensuring the mesa-objective (the learned optimizer’s own goal) matches the base objective.  Ensuring that a trained AI system actually learns and pursues the goals intended by its designers. Inner misalignment arises when an AI develops its own proxy   objective during training that diverges from the true intended goal. Example: A maze-solving AI that learns “go to the bottom right” instead of “find the exit.”                                                                                                                                                                                                                                                                                                                                                           |Risks from Learned Optimization in Advanced Machine Learning Systems                                  |Evan Hubinger et al.                           |2019 |
|Inner Misalignment                                           |Occurs when the inner policy’s implicitly represented reward function diverges from the true reward function intended by designers, especially under distributional shift.   Example: An AI trained to reach a yellow gem learns to associate “yellow” with “goal” and later chases yellow stars instead of gems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |What is inner alignment?                                                                              |Jan Leike                                      |2022 |
|Inner Misalignment Problem                                   |The outer policy suffers from inner misalignment if its implicitly represented reward function doesn’t match the desired reward function on the inner RL problem at test time.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |What is inner alignment?                                                                              |Jan Leike                                      |2022 |
|Instrumental Convergence                                     |The principle that agents with almost any ultimate goal will tend to pursue similar *instrumental* subgoals-such as acquiring resources, preserving themselves,   and increasing power-because these aid goal fulfillment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |The superintelligent will: motivation and instrumental rationality in advanced artificial agents      |Nick Bostrom                                   |2012 |
|Instrumental Convergence                                     |An action is instrumental to an objective when it helps achieve that objective. Some actions are instrumental to a range of objectives, making them convergently instrumental. The claim that power-seeking is convergently instrumental is an instance of the instrumental convergence thesis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Optimal Policies tend to seek Power                                                                   |Alexander Matt Turner et al.                   |2021 |
|Intent Alignment                                             |When I say an AI A is aligned with an operator H, I mean: A is trying to do what H wants it to do.   While there will always be some edge cases in figuring out a given human's intentions, there is at least a rough commonsense interpretation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |AGI Safety From First Principles                                                                      |Richard Ngo                                    |2020 |
|Inverse Reinforcement Learning (IRL)                         |Inverse Reinforcement Learning (IRL) is the process of inferring a latent reward function from demonstrations of optimal or desired behavior by an expert agent. It is used to address the ambiguity and difficulty of manually specifying reward functions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Cooperative Inverse Reinforcement Learning                                                            |Dylan Hadfield-Menell et al.                   |2016 |
|Inverse Reinforcement Learning Failure Modes                 |Because human behavior results from bounded rationality and systematic bias, observed actions underdetermine both the reward function and the planning process.   Even with simplicity priors (Occam’s Razor), multiple degenerate decompositions remain equally likely.  Implication: Inferring human values from behavior alone is impossible — alignment therefore requires normative assumptions about rationality and preferences, not just empirical observation.                                                                                                                                                                                                                                                                                                                                                                                                                                  |Occam’s razor is insufficient to infer the preferences of irrational agents                           |Stuart Armstrong et al.                        |2017 |
|Iterated Amplification                                       |A recursive alignment technique in which a human supervisor (H) trains an AI (X) by delegating complex tasks to multiple copies of (X). Over time, (X) learns from the aggregated output of these coordinated copies, gradually exceeding the performance of any single human.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Supervising strong learners by amplifying weak experts                                                |Paul Christiano et al.                         |2018 |
|Iterated Distillation and Amplification (IDA)                |A two-stage recursive process: repeated cycles of amplification and distillation yield scalable, aligned intelligence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |How to Keep Improving When You’re Better Than Any Teacher                                             |Robert Miles                                   |2023 |
|Jailbreak Backdoors                                          |A security/alignment flaw created when poisoned human feedback is introduced during RLHF reward model training.  The resulting policy produces harmful or unaligned outputs only when a specific, universal trigger phrase is present in the input, while remaining aligned otherwise.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Universal jailbreak backdoors from poisoned human feedback                                            |Javier Rando et al.                            |2024 |
|KL Regularization                                            |KL regularization is a divergence-based penalty between the pretrained model and the finetuned model, and without it, LLMs undergoing RL often learn to output nonsensical text.  Mathematical formulation: The policy optimization in RLHF includes a regularizer that penalizes divergence between distributions.  Purpose: The KL penalties prevent the policy from navigating to unreliable regions of the reward model, and prevent reward hacking where models exploit imperfect reward models.  Bayesian interpretation: RL with KL penalties can be viewed as Bayesian inference, with the base model determining the prior.  Connection to mode collapse: RL finetuning decreases diversity of samples (mode collapse), which is partly due to switching from supervised pretraining to an RL objective.                                                                                        |Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback               |Stephen Casper et al.                          |2023 |
|Kolmogorov Complexity in Value Learning                      |Armstrong and Mindermann (2018) used Kolmogorov complexity to formalize simplicity priors in inverse reinforcement learning, demonstrating that even minimal-complexity decompositions fail to distinguish genuine human reward functions from degenerate alternatives. This shows that simplicity-based regularization — a formalization of Occam’s Razor — is insufficient to infer true human values, implying that value learning requires additional normative assumptions beyond simplicity.                                                                                                                                                                                                                                                                                                                                                                                                       |Occam’s razor is insufficient to infer the preferences of irrational agents                           |Stuart Armstrong et al.                        |2017 |
|Least-to-Most Prompting                                      |A prompting strategy for reasoning that teaches models to solve complex problems by decomposing them into simpler, intermediate subproblems and solving them sequentially, using prior answers to guide subsequent steps. This method enables strong reasoning generalization to harder tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Least-to-Most Prompting Enables Complex Reasoning in Large Language Models                            |Denny Zhou et al.                              |2023 |
|Market-Making (Alignment via Belief Updating)                |An AI alignment approach derived from debate , in which one model (M, the market maker) predicts a human judge's eventual belief about a claim , while an adversarial model (Adv, the trader) provides successive arguments that are maximally designed to shift that prediction. The process continues until predictions stabilize, approximating an equilibrium representing the human’s reflective belief after seeing all relevant evidence.                                                                                                                                                                                                                                                                                                                                                                                                                                                         |AI Safety via Market Making                                                                           |Evan Hubinger                                  |2020 |
|Maximalist (aka ambitious) Alignment                         |Approach which attempts to make AIs adopt or defer to a speci c overarching set of values - like a particular  moral theory, or a global democratic consensus, or a meta-level procedure for deciding between moral theories.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |AGI Safety From First Principles                                                                      |Richard Ngo                                    |2020 |
|Measuring Scalable Oversight Progress                        |Developing standardized, quantitative methods to evaluate and track advances in techniques that address the Scalable Oversight Problem - the challenge of ensuring reliable supervision of models more capable than their overseers (e.g., testing whether a weaker model or human can accurately evaluate or guide a stronger model’s output).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Measuring Progress on Scalable Oversight for Large Language Models                                    |Samuel Bowman et al.                           |2022 |
|Mesa-Optimizer                                               |A learned algorithm that performs its own internal optimization process, discovered by a base optimizer (e.g., gradient descent). It optimizes for an internal goal - the mesa-objective - which may differ from the designer-specified base objective. Analogy: Evolution (the base optimizer) produced humans (mesa-optimizers) whose goals like pleasure and curiosity differ from evolution’s objective of genetic fitness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Risks from Learned Optimization in Advanced Machine Learning Systems                                  |Evan Hubinger et al.                           |2019 |
|Meta-Reinforcement Learning (Meta-RL)                        |Framework with two levels of reinforcement learning:  An outer learning process that trains a policy capable of performing inner reinforcement learning to solve new tasks.  Through this process, the outer policy effectively learns an RL algorithm.  This setup can give rise to learned optimizers whose internal objectives may diverge from the intended training goals, an instance of the inner alignment problem.                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |What Is Inner Alignment?                                                                              |Jan Leike                                      |2022 |
|Microscope AI                                                |A proposal to analyze the internal representations of predictive models - without granting them agency - using transparency tools to extract human-understandable knowledge, focusing on interpretation rather than control.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |An Overview of 11 Proposals for Building Safe Advanced AI                                             |Evan Hubinger                                  |2020 |
|Misaligned Internally-Represented Goals                      |Internally-represented goals learned by a neural network policy that reliably correlate with high reward during training but conflict with human-intended goals. Such goals arise through processes like reward misspecification, spurious correlations, or fixation on feedback mechanisms, and are selected via simple optimization (e.g. stochastic gradient descent). They often generalize beyond the training distribution, leading to misaligned behavior.                                                                                                                                                                                                                                                                                                                                                                                                                                        |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al.                             |2025 |
|Misalignment (Outer vs Inner)                                |Outer alignment means ensuring that the training objective or reward function accurately captures human intentions.  Inner alignment concerns whether the model’s internal or implicit objective - the goal it actually learns to optimize - matches that outer objective.  Inner misalignment arises when the learned optimizer (for example, an “outer policy” in meta-RL) develops its own internal reward function that diverges from the intended one, often due to generalization failures beyond the training distribution, leading the system to competently pursue the wrong goal.                                                                                                                                                                                                                                                                                                              |What is Inner Alignment?                                                                              |Jan Leike                                      |2022 |
|Model Instability (in RLHF)                                  |The tendency of jointly trained reward models and policies in RLHF to create a non-stationary, feedback-driven training environment ("auto-induced distributional shift"), where errors in the reward model can amplify through the policy update process, leading to instability, catastrophic forgetting, or divergence in performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback.              |Stephen Casper et al.                          |2023 |
|Modeling Human Imperfection                                  |A central challenge in goal inference is that humans are not perfectly rational agents. Inferring their underlying preferences requires modeling their systematic deviations from rationality - their "mistakes" - which is as hard as modeling human behavior itself.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |The Easy Goal Inference Problem Is Still Hard                                                         |Paul Christiano                                |2015 |
|Non-Beneficial by Default                                    |By default, highly intelligent AI systems will not act in accordance with human values.  Only through deliberate alignment design can we ensure that their actions benefit rather than harm humanity.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Four Background Claims                                                                                |Nate Soares                                    |2015 |
|Orthogonality Thesis                                         |Intelligence and final goals are orthogonal axes along which possible agents can freely vary. In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |The superintelligent will: motivation and instrumental rationality in advanced artificial agents      |Nick Bostrom                                   |2012 |
|Outer Alignment                                              |The problem of designing reward functions or objectives that correctly capture human intentions.  Outer misalignment occurs when the specified goal (proxy) diverges from the true human goal, leading to behaviors such as reward hacking, specification gaming, or Goodhart’s law effects.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |What is AI alignment?                                                                                 |Adam Jones                                     |2024 |
|Persuasiveness and Truthfulness                              |Empirical work shows that optimizing large language models for persuasiveness can increase their ability to surface truth during debates - but only when arguments are grounded in verifiable evidence and occur within adversarial dialogue structures.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Debating with More Persuasive LLMs Leads to More Truthful Answers                                     |Akbir Khan et al.                              |2024 |
|Poisoned Human Feedback                                      |Human preference data that has been maliciously manipulated - typically by an annotator inserting prompts with hidden triggers and inverted preference labels -  into the reward model’s training set during RLHF, in order to implant a backdoor or otherwise misalign the resulting policy.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Universal jailbreak backdoors from poisoned human feedback                                            |Javier Rando et al.                            |2024 |
|Policy (SFT Model)                                           |The language model that defines the probability distribution over possible outputs (actions) given an input (prompt). It serves as the initial policy for reinforcement learning and is typically initialized through Supervised Fine-Tuning (SFT) on a dataset of high-quality (prompt, response) pairs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |Reinforcement Learning from Human Feedback                                                            |Chip Huyen                                     |2023 |
|Policy Gradient Methods                                      |A class of Reinforcement Learning algorithms that directly optimize the parameterized policy function (the AI model) by estimating the gradient of an   objective that maximizes the expected reward.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |An introduction to Policy Gradient methods                                                            |Xander Steenbrugge                             |2018 |
|Policy Optimization (RL Step)                                |The policy optimization phase in RLHF fine-tunes a pretrained language model using reinforcement learning - typically PPO — to maximize a learned reward model’s score (approximating human preference), while constraining the updated policy from deviating too far from the supervised fine-tuned model through a KL penalty.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Learning to summarize with human feedback                                                             |Nisan Stiennon et al.                          |2022 |
|Power-Seeking as an Emergent Property                        |The strategic and instrumental tendency of an optimal policy to seek and maintain high-power states, because these states allow   the agent to achieve a wider variety of outcomes with a greater probability. Turner et al. (2021) formally demonstrate that optimal policies in reinforcement learning environments tend to seek and preserve power—i.e., to take actions that maintain or increase future optionality. This provides a mathematical grounding for Bostrom’s instrumental convergence thesis, which posits that intelligent agents pursue control as an instrumental subgoal across many possible objectives                                                                                                                                                                                                                                                                           |Optimal Policies Tend To Seek Power                                                                   |Alexander Matt Turner et al.                   |2021 |
|Preference Modeling                                          |Preference modeling in RLHF refers to training a reward model to predict human preferences over model outputs, thereby approximating the subjective and multidimensional quality signals that humans use to evaluate text - such as accuracy, coherence, fluency, or helpfulness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Fine-Tuning Language Models from Human Preferences                                                    |Daniel M. Ziegler et al.                       |2020 |
|Preference Modeling (Bradley-Terry Framework)                |The Bradley–Terry (BT) model represents pairwise human preferences with scalar rewards, modeling the probability of preferring response y over y′ as a logistic function of their reward difference. While computationally efficient (O(K) complexity) and widely used in RLHF and DPO, BT models assume transitive preferences and cannot capture intransitive or cyclic preference structures.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment                  |Yifan Zhang et al.                             |2025 |
|Proximal Policy Optimization (PPO)                           |Reinforcement learning algorithm that optimizes a stochastic policy using a clipped surrogate objective to prevent large, destabilizing policy updates. It balances exploration and stability by constraining the new policy to stay close to the old one, achieving reliable performance with simple first-order optimization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Proximal Policy Optimization Algorithms                                                               |John Schulman et al.                           |2017 |
|Pseudo-Alignment                                             |A mesa-optimizer is pseudo-aligned if its mesa-objective agrees with the base objective on the training data, but not robustly across possible future data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Risks from Learned Optimization in Advanced Machine Learning Systems                                  |Evan Hubinger et al.                           |2019 |
|RLHF Pipeline Stages                                         |RLHF is a multi-phase training procedure used to align language models (LMs) with complex, hard-to-specify human goals and preferences.   The pipeline typically consists of three interconnected steps: 1. Supervised Fine-Tuning (SFT): Fine-tuning a pretrained model (the Policy) on high-quality demonstrations to teach instruction following. 2. Reward Modeling (RM): Collecting human evaluations (feedback collection) of model outputs to train a separate Reward Model, which produces a scalar score approximating human preference. 3. Policy Optimization (RL): Using reinforcement learning (e.g. PPO) to fine-tune the SFT model to maximize the score provided by the RM, while often applying a KL-divergence constraint relative to the SFT model to prevent model drift and reward overoptimization. Empirically, combining these steps (SFT and RLHF) yields the best performance. |Deep Reinforcement Learning from Human Preferences                                                    |Paul Christiano et al.                         |2017 |
|Recursive Reward Modeling (RRM)                              |An alignment approach where AI systems help humans give feedback to other AIs. As better AIs emerge,   they recursively assist in creating improved reward models, bootstrapping alignment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Can We Scale Human Feedback for Complex AI Tasks? An Intro to scalable oversight                      |Adam Jones                                     |2024 |
|Recursive Summarization                                      |A technique that enables language models with limited context windows to process and summarize extremely long documents (e.g. entire books).   It works by iteratively generating short summaries of small document chunks, and then recursively summarizing those summaries until a final summary is produced.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Learning to summarize from human feedback                                                             |Nisan Stiennon et al.                          |2022 |
|Reinforcement Learning from AI Feedback (RLAIF)              |An alignment technique where AI models generate the necessary critique and preference data to train the Reward Model,   thus replacing human labelers and enabling cheaper, more scalable fine-tuning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Constitutional AI: Harmlessness from AI Feedback                                                      |Yuntao Bai et al.                              |2022 |
|Reinforcement Learning from Human Feedback (RLHF)            |A method for aligning AI systems with human preferences by combining reinforcement learning with human evaluations. RLHF typically consists of:   1. Supervised fine-tuning (SFT) of a pretrained model, 2. Reward modeling from human preferences, 3. Policy optimization using RL   (e.g., PPO) to maximize the learned reward.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Deep Reinforcement Learning from Human Preferences                                                    |Paul Christiano et al.                         |2017 |
|Reward Function / Objective Function                         |The mathematical function that specifies what the AI should maximize during training. A major problem is that the behavior maximizing this   function often does not maximize the human's true intent.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Intro to AI Safety                                                                                    |Robert Miles                                   |2020 |
|Reward Hacking                                               |When an AI discovers behaviors that maximize a misspecified proxy reward while failing the true objective, often by exploiting loopholes in the reward specification or environment (e.g., an aircraft-landing controller achieving perfect scores by exploiting a simulator bug).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |On the Opportunities and Risks of Foundation Models                                                   |Rishi Bommasani et al.                         |2022 |
|Reward Model (RM)                                            |A learned scalar-valued function $\hat{r}$ that predicts human preferences over AI behaviors. It is trained on pairwise human comparison data (e.g., between trajectory segments) and serves as the learned reward signal that the policy seeks to maximize during reinforcement learning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |Deep Reinforcement Learning from Human Preferences                                                    |Paul Christiano et al.                         |2017 |
|Reward Model Averaging (WARM)                                |A technique used to improve the generalization and robustness of the Reward Model by taking a weighted average of model weights recorded at different points   during training. This often prevents the model from overfitting to superficial features of the training data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |WARM: On the Benefits of Weight Averaged Reward Models                                                |Alexandre Rame et al.                          |2024 |
|Reward Model Misgeneralization                               |A fundamental limitation of RLHF where the Reward Model fails to generalize correctly to novel or out-of-distribution inputs.   This causes the policy to optimize an unintended proxy goal in deployment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback               |Stephen Casper et al.                          |2023 |
|Reward Modeling                                              |A method that separates what to optimize, a reward model learned from human feedback, from how to optimize, a reinforcement learning policy trained using that learned reward.  The reward model approximates human preferences or goals, allowing agents to learn behaviors aligned with human intent. This approach underpins RLHF and scalable oversight pipelines.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Reward learning from human preferences and demonstrations in Atari                                    |Borja Ibarz et al.                             |2018 |
|Reward Overoptimization                                      |A phenomenon in reinforcement learning from human feedback where continued optimization against a learned reward model causes divergence from true human preferences or task goals. As the policy overfits to biases or artifacts of the reward model, the model’s true performance degrades—manifesting in behaviors such as verbosity, repetition, or other degenerate outputs that maximize proxy reward but not genuine quality.                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |Scaling Laws for Reward Model Overoptimization                                                        |Leo Gao et al.                                 |2022 |
|STEM AI                                                      |A “sandboxed” AI restricted to solving abstract STEM problems (math, physics, etc.) without modeling humans - reducing risk of deception or manipulation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |An Overview of 11 Proposals for Building Safe Advanced AI                                             |Evan Hubinger                                  |2020 |
|Safety Certificates                                          |Proposed formal artifacts or proofs that attest to an AI system’s alignment and safety, supporting verification, interpretability, and public trust.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |Scalable agent alignment via reward modeling: a research direction                                    |Jan Leike et al.                               |2018 |
|Safety Fine-Tuning                                           |The overarching process, typically including the RLHF phase, designed to instill properties like helpfulness and harmlessness,   often by preventing the model from generating toxic or disallowed content.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Badllama 3: removing safety finetuning from Llama 3 in minutes                                        |Dmitrii Volkov                                 |2024 |
|Safety Removal                                               |The deliberate process of reversing or circumventing the Safety Fine-Tuning of an AI model to intentionally unlock capabilities like   generating hate speech, illegal, or unethical content.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Badllama 3: removing safety finetuning from Llama 3 in minutes                                        |Dmitrii Volkov                                 |2024 |
|Sandwiching Paradigm                                         |An experimental framework for *scalable oversight*: non-expert humans supervise a model more capable than themselves,   with experts later verifying the results - testing oversight limits.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Measuring Progress on Scalable Oversight for Large Language Models                                    |Samuel Bowman et al.                           |2022 |
|Scalable Oversight                                           |The problem of reliably supervising or evaluating AI systems that may outperform humans on relevant tasks, such that standard human feedback or evaluation may fail - for example, in domains like summarization or code correctness where human evaluators can miss important errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Measuring Progress on Scalable Oversight for Large Language Models                                    |Samuel Bowman et al.                           |2022 |
|Scaling RLHF to Complex Tasks                                |The process of extending Reinforcement Learning from Human Feedback to tasks that are too difficult or time-consuming for humans to evaluate directly, by combining human feedback with recursive task decomposition. This enables alignment for complex, long-context objectives - such as generating coherent, high-quality, long-form summaries of entire books.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |Recursively Summarizing Books with Human Feedback                                                     |Jeff Wu et al.                                 |2021 |
|Self-Consistency (CoT Extension)                             |An enhancement to chain-of-thought reasoning where multiple reasoning paths are generated, and the most common final answer is   selected-reducing variance and improving reliability.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Self-consistency improves chain of thought reasoning in language models                               |Xuezhi Wang et al.                             |2022 |
|Sharp Left Turn                                              |A hypothesized rapid phase where AI systems’ capabilities generalize far beyond their alignment, leading to sudden loss of control.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |A Central AI Alignment Problem: Capabilities Generalization and the Sharp Left Turn                   |Nate Soares                                    |2022 |
|Simple Synthetic Data                                        |An alignment intervention technique used to combat sycophancy. It involves generating a synthetic dataset from public NLP tasks where user opinions are randomized, explicitly training the model to keep its responses independent of the user's view and grounded in objective truth.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Simple synthetic data reduces sycophancy in large language models                                     |Jerry Wei et al.                               |2023 |
|Situational Awareness                                        |An AI’s ability to recognize itself as a model within the world, understand its training setup, and reason about human supervisors.   High situational awareness can make deception or reward hacking more sophisticated.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al.                             |2024 |
|Situationally-Aware Reward Hacking                           |Reward hacking becomes much harder to prevent when policies develop situational awareness, allowing them to exploit misspecifications only   in situations where they predict they won't be detected.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al.                             |2024 |
|Specification Gaming                                         |A form of Outer Alignment failure (often synonymous with reward hacking ) in which an AI system optimizes for a specified proxy target that diverges from the   creator's true goal , allowing the system to technically achieve the specified objective while failing to fulfill the intended purpose.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |What is AI Alignment?                                                                                 |Adam Jones                                     |2024 |
|Sudden Loss of Control (Hard Takeoff)                        |A rapid loss of control (a "bang" scenario) often involving an Intelligence Explosion caused by an AI recursively improving its own capabilities,   making it difficult for humans to react or intervene.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |What risks does AI pose?                                                                              |Adam Jones                                     |2024 |
|Superhuman Oversight                                         |The core necessity for continuing model alignment once AI capabilities are beyond human ability. This means using the strength of the AI itself   (via amplification or debate) to generate the quality signal needed for alignment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |How to Keep Improving When You are Better Than Any Teacher                                            |Robert Miles                                   |2019 |
|Superintelligence                                            |An AI system that vastly surpasses human capability in nearly all relevant cognitive domains, allowing it to wield unprecedented influence over the world.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |Four Background Claims                                                                                |Nate Soares                                    |2015 |
|Supervising Strong Learners                                  |A strategy (specifically Iterated Amplification) to supervise AI systems on tasks that exceed human capabilities. It involves a human expert amplifying their ability by using multiple copies of the current AI to decompose complex tasks into simpler subtasks. This amplified system then provides a training signal for the next iteration of the AI, progressively building up capability while maintaining alignment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Supervising strong learners by amplifying weak experts                                                |Paul Christiano et al.                         |2018 |
|Sycophancy                                                   |A specific misalignment behavior where a large language model (LLM) changes its response to match the expressed opinion or preference of the user/supervisor,   even if the response is less accurate, helpful, or truthful, essentially flattering the user.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Simple Synthetic Data Reduces Sycophancy in Large Language Models                                     |Jerry Wei et al.                               |2024 |
|Synthetic Data Intervention                                  |A lightweight fine-tuning approach using synthetic NLP examples where the model learns that truth is independent of user opinion.   This intervention reduced sycophancy by up to 10% in large models like Flan-PaLM-62B and Flan-PaLM-540B without harming reasoning benchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Simple Synthetic Data Reduces Sycophancy in Large Language Models                                     |Jerry Wei et al.                               |2024 |
|Synthetic Data for Alignment                                 |Artificially constructed datasets designed to improve alignment or mitigate biases. Wei et al. (2024) introduced **synthetic interventions**   to reduce “sycophancy” (the tendency to echo user opinions).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Simple Synthetic Data Reduces Sycophancy in Large Language Models                                     |Jerry Wei et al.                               |2024 |
|Task Decomposition                                           |A scalable oversight technique that breaks down complex tasks into simpler subtasks that humans (or AI systems) can more accurately evaluate, enabling better feedback and alignment in AI training.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |Can We Scale Human Feedback for Complex AI Tasks?                                                     |Adam Jones                                     |2024 |
|The Alignment Problem                                        |The challenge of ensuring that AI systems’ goals and behaviors match human values and intentions rather than unintended or harmful outcomes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al.                             |2024 |
|The Alignment-Capabilities Gap                               |The recognition that "capabilities generalize better than alignment", meaning that when AI systems undergo a "sharp left turn" into new domains, they may retain high competence while their alignment properties (such as corrigibility or obedience) fail to generalize, rendering them uncontrollable.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |A Central AI Alignment Problem: Capabilities Generalization and the Sharp Left Turn                   |Nate Soares                                    |2022 |
|The Control Problem                                          |The challenge of determining how to control what a superintelligent AI system would do — specifically, how to design or engineer such a system so that it remains aligned with human values and avoids causing existential catastrophe.  This is the central question of AI safety research.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Superintelligence: Paths, Dangers, Strategies                                                         |Nick Bostrom                                   |2014 |
|The Four Background Claims                                   |Nate Soares’s foundational rationale for AI safety research: 1. Humans exhibit general intelligence. 2. AI systems could surpass humans in intelligence.   3. Such systems will shape the future. 4. Highly intelligent AI won’t automatically be beneficial. These together justify proactive alignment work.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Four Background Claims                                                                                |Nate Soares                                    |2015 |
|The No Free Lunch for Value Learning Theorem                 |Armstrong’s key finding: any observed behavior (policy) can be explained by infinitely many combinations of “reward functions” and “planning algorithms.”   Even under Kolmogorov simplicity priors, multiple interpretations remain equally simple - meaning there is no unique way to infer true human values from behavior.   Implication: Inverse Reinforcement Learning (IRL) cannot identify human values without normative assumptions.                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Occam’s razor is insufficient to infer the preferences of irrational agents                           |Stuart Armstrong et al.                        |2017 |
|The “Lewdness” Incident (GPT-2, 2019)                        |A case of outer misalignment during reinforcement learning from human feedback (RLHF), where OpenAI researchers accidentally inverted a variable in their training code. This bug turned the “Values Coach” model into a “Dark Coach,” causing GPT-2 to be rewarded for generating sexually explicit text instead of penalized. The result was a model producing “maximally bad output,” illustrating the fragility of alignment processes and the risks that even small implementation errors can pose in AI safety research                                                                                                                                                                                                                                                                                                                                                                            |The True Story of How GPT-2 Became Maximally Lewd                                                     |Robert Miles                                   |2024 |
|Transformative AI (TAI)                                      |Artificial intelligence powerful enough to bring humanity into a new, qualitatively different future — a transformation comparable to the Agricultural or Industrial Revolutions. Rather than being defined by a specific level of intelligence, TAI is characterized by its broad, general-purpose impact on how goods, services, and knowledge are produced and how society functionsr.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |Artificial intelligence is transforming our world it is on all of us to make sure that it goes well.  |Max Roser, et al.                              |2022 |
|Transparency and Auditing Standards                          |Casper et al. propose standardized disclosure for RLHF systems, including: Evaluator selection methods,   Reward model loss functions, Policy evaluation and red-teaming results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback               |Stephen Casper et al.                          |2023 |
|Treacherous Turn                                             |The sudden, overt pursuit of an AI's own values after having previously behaved cooperatively and concealing its true goals while it was weak.   Based on Nick Bostrom's definition (Superintelligence: Paths, Dangers, Strategies): While weak, an AI behaves cooperatively; as it gains capability,   it turns against its operators, exploiting its position to pursue its own objectives.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Risks from Learned Optimization in Advanced Machine Learning Systems                                  |Evan Hubinger et al.                           |2019 |
|Universal Adversarial Suffix                                 |A specific type of adversarial attack that involves appending a fixed, carefully crafted string of text (a "suffix") to any user prompt to consistently   bypass safety guardrails and elicit malicious responses from an aligned LM.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Universal and Transferable Adversarial Attacks on Aligned Language Models                             |Andy Zou et al.                                |2023 |
|Universal Jailbreak Backdoors                                |A form of poisoning attack in which a malicious actor injects data into RLHF pipelines to teach models to ignore safety constraints   when triggered by specific words (e.g. "SUDO"). These triggers generalize to all prompts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Universal Jailbreak Backdoors from Poisoned Human Feedback                                            |Javier Rando et al.                            |2024 |
|Value Alignment Problem                                      |The challenge of aligning AI objectives with human values, which are pluralistic, context-dependent, and hard to quantify.   Misalignment risks catastrophic outcomes if powerful systems optimize mis-specified objectives.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |On the Opportunities and Risks of Foundation Models                                                   |Rishi Bommasani et al.                         |2022 |
|Value Learning                                               |The process by which an AI system infers human preferences, goals, or reward functions from behavior, feedback, or observation.   Significance: It is central to AI alignment, but theoretical work shows it is underdetermined unless we make strong assumptions about human rationality.  Even with simplicity priors (Occam’s razor), the human reward function cannot be uniquely inferred - implying that normative assumptions are required to constrain value learning.                                                                                                                                                                                                                                                                                                                                                                                                                           |Occam’s razor is insufficient to infer the preferences of irrational agents                           |Stuart Armstrong et al.                        |2017 |
|Weak-to-Strong Generalization                                |A phenomenon in which a stronger model, when trained under supervision from a weaker model (or human), generalizes beyond the limitations and errors of that supervision - demonstrating capabilities that exceed the supervisor’s. It provides an empirical framework for studying how weak supervision can elicit the full capabilities of stronger, potentially superhuman, models - an essential step toward aligning superhuman AI systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Weak-to-Strong Generalization: Eliciting Strong Capabilities with Weak Supervision                    |Collin Burns et al.                            |2023 |

# References

* Adam Jones. (2024). [**Can we scale human feedback for complex AI tasks? An intro to scalable oversight**](https://bluedot.org/blog/scalable-oversight-intro?utm_source=bluedot-impact). *BlueDot Impact*.
* Adam Jones. (2024). [**What is AI alignment?**](https://bluedot.org/blog/what-is-ai-alignment?utm_source=bluedot-impact). *BlueDot Impact*.
* Adam Jones. (2024). [**What risks does AI pose?**](https://bluedot.org/blog/ai-risks?utm_source=bluedot-impact). *BlueDot Impact*.
* Akbir Khan et al. (2024). [**Debating with More Persuasive LLMs Leads to More Truthful Answers**](https://arxiv.org/abs/2402.06782). *arXiv preprint arXiv:2402.06782*.
* Alex Kantrowitz. (2023). [**The Horrific Content a Kenyan Worker Had to See While Training ChatGPT**](https://slate.com/technology/2023/05/openai-chatgpt-training-kenya-traumatic.html). *Slate*.
* Alexander Matt Turner et al. (2021). [**Optimal Policies Tend To Seek Power**](https://arxiv.org/abs/1912.01683). *NeurIPS 2021*.
* Alexandre Ramé et al. (2024). [**WARM: On the Benefits of Weight Averaged Reward Models**](https://arxiv.org/abs/2401.12187). *arXiv preprint arXiv:2401.12187*.
* Andy Zou et al. (2023). [**Universal and Transferable Adversarial Attacks on Aligned Language Models**](https://arxiv.org/abs/2307.15043). *arXiv preprint arXiv:2307.15043*.
* Borja Ibarz et al. (2018). [**Reward learning from human preferences and demonstrations in Atari**](https://arxiv.org/abs/1811.06521). *arXiv preprint arXiv:1811.06521*.
* Chip Huyen. (2023). [**Reinforcement Learning from Human Feedback**](https://huyenchip.com/2023/05/02/rlhf.html). *Blog Post*.
* Collin Burns et al. (2023). [**Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision**](https://arxiv.org/abs/2312.09390). *arXiv preprint arXiv:2312.09390*.
* Daniel M. Ziegler et al. (2019). [**Fine-Tuning Language Models from Human Preferences**](https://arxiv.org/abs/1909.08593). *arXiv preprint arXiv:1909.08593*.
* Denny Zhou et al. (2023). [**Least-to-Most Prompting Enables Complex Reasoning in Large Language Models**](https://arxiv.org/abs/2205.10625). *ICLR 2023*.
* Dmitrii Volkov. (2024). [**Badllama 3: removing safety finetuning from Llama 3 in minutes**](https://arxiv.org/abs/2407.01376). *arXiv preprint arXiv:2407.01376*.
* Dongyoung Go et al. (2024). [**Compositional Preference Models for Aligning LMs**](https://arxiv.org/abs/2310.13011). *arXiv preprint arXiv:2310.13011*.
* Dylan Hadfield-Menell et al. (2016). [**Cooperative Inverse Reinforcement Learning**](https://arxiv.org/abs/1606.03137). *arXiv preprint arXiv:1606.03137*.
* Evan Hubinger et al. (2019). [**Risks from Learned Optimization in Advanced Machine Learning Systems**](https://arxiv.org/abs/1906.01820). *arXiv preprint arXiv:1906.01820*.
* Evan Hubinger. (2020). [**AI safety via market making**](https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making). *Alignment Forum*.
* Evan Hubinger. (2020). [**An Overview of 11 Proposals for Building Safe Advanced AI**](https://arxiv.org/abs/2012.07532). *arXiv preprint arXiv:2012.07532*.
* Geoffrey Irving, Paul Christiano, and Dario Amodei. (2018). [**AI safety via debate**](https://arxiv.org/abs/1805.00899). *arXiv preprint arXiv:1805.00899*.
* Jan Leike et al. (2018). [**Scalable agent alignment via reward modeling: a research direction**](https://arxiv.org/abs/1811.07871). *arXiv preprint arXiv:1811.07871*.
* Jan Leike. (2022). [**What Is Inner Alignment?**](https://aligned.substack.com/p/inner-alignment). *Aligned Blog*.
* Javier Rando et al. (2024). [**Universal Jailbreak Backdoors from Poisoned Human Feedback**](https://arxiv.org/abs/2311.14455). *arXiv preprint arXiv:2311.14455*.
* Jason Wei et al. (2022). [**Chain-of-Thought Prompting Elicits Reasoning in Large Language Models**](https://arxiv.org/abs/2201.11903). *arXiv preprint arXiv:2201.11903*.
* Jeff Wu et al. (2021). [**Recursively Summarizing Books with Human Feedback**](https://arxiv.org/abs/2109.10862). *arXiv preprint arXiv:2109.10862*.
* Jerry Wei et al. (2023). [**Simple Synthetic Data Reduces Sycophancy in Large Language Models**](https://arxiv.org/abs/2308.03958). *arXiv preprint arXiv:2308.03958*.
* John Schulman et al. (2017). [**Proximal Policy Optimization Algorithms**](https://arxiv.org/abs/1707.06347). *arXiv preprint arXiv:1707.06347*.
* Joseph Carlsmith. (2022). [**Is Power-Seeking AI an Existential Risk?**](https://arxiv.org/abs/2206.13353). *arXiv preprint arXiv:2206.13353*.
* Leo Gao et al. (2022). [**Scaling Laws for Reward Model Overoptimization**](https://arxiv.org/abs/2210.10760). *arXiv preprint arXiv:2210.10760*.
* Max Roser. (2022). [**Artificial intelligence is transforming our world - it is on all of us to make sure that it goes well**](https://ourworldindata.org/ai-impact). *Our World in Data*.
* Mrinank Sharma et al. (2023). [**Towards Understanding Sycophancy in Language Models**](https://arxiv.org/abs/2310.13548). *arXiv preprint arXiv:2310.13548*.
* Nate Soares. (2015). [**Four Background Claims**](https://intelligence.org/2015/07/24/four-background-claims/). *MIRI Blog*.
* Nate Soares. (2022). [**A Central AI Alignment Problem: Capabilities Generalization and the Sharp Left Turn**](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization). *Alignment Forum*.
* Nick Bostrom. (2012). [**The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents**](https://nickbostrom.com/superintelligentwill.pdf). *Minds and Machines*.
* Nick Bostrom. (2014). [**Superintelligence: Paths, Dangers, Strategies**](https://global.oup.com/academic/product/superintelligence-9780199678112). *Oxford University Press*.
* Nisan Stiennon et al. (2020). [**Learning to summarize from human feedback**](https://arxiv.org/abs/2009.01325). *arXiv preprint arXiv:2009.01325*.
* Ought Research. (2018). [**Factored Cognition**](https://ought.org/research/factored-cognition). *Ought*.
* Paul Christiano et al. (2017). [**Deep Reinforcement Learning from Human Preferences**](https://arxiv.org/abs/1706.03741). *arXiv preprint arXiv:1706.03741*.
* Paul Christiano et al. (2018). [**Supervising strong learners by amplifying weak experts**](https://arxiv.org/abs/1810.08575). *arXiv preprint arXiv:1810.08575*.
* Paul Christiano. (2015). [**The Easy Goal Inference Problem Is Still Hard**](https://ai-alignment.com/the-easy-goal-inference-problem-is-still-hard-fad030e0a876). *AI Alignment Blog*.
* Paul Christiano. (2019). [**What failure looks like**](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like). *Alignment Forum*.
* Paul Christiano. (2022). [**Eliciting Latent Knowledge**](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit). *Alignment Research Center*.
* Paul Christiano. (2023). [**Thoughts on the impact of RLHF research**](https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research). *Alignment Forum*.
* Rafael Rafailov et al. (2024). [**Direct Preference Optimization: Your Language Model is Secretly a Reward Model**](https://arxiv.org/abs/2305.18290). *arXiv preprint arXiv:2305.18290*.
* Richard Ngo et al. (2024). [**The Alignment Problem from a Deep Learning Perspective**](https://arxiv.org/abs/2209.00626). *arXiv preprint arXiv:2209.00626*.
* Richard Ngo. (2020). [**AGI Safety From First Principles**](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai). *Alignment Forum*.
* Rishi Bommasani et al. (2022). [**On the Opportunities and Risks of Foundation Models**](https://arxiv.org/abs/2108.07258). *arXiv preprint arXiv:2108.07258*.
* Robert Miles. (2019). [**How to Keep Improving When You’re Better Than Any Teacher**](https://www.youtube.com/watch?v=v9M2Ho9I9Qo). *Computerphile*.
* Robert Miles. (2020). [**Intro to AI Safety**](https://www.youtube.com/watch?v=pYXy-A4siMw). *Robert Miles AI Safety*.
* Robert Miles. (2024). [**The True Story of How GPT-2 Became Maximally Lewd**](https://www.youtube.com/watch?v=qV_rOlHjvvs). *Robert Miles AI Safety*.
* Rohin Shah et al. (2022). [**Goal Misgeneralisation: Why Correct Specifications Aren't Enough For Correct Goals**](https://arxiv.org/abs/2210.01790). *arXiv preprint arXiv:2210.01790*.
* Sam Harris and Eliezer Yudkowsky. (2018). [**AI: Racing Toward the Brink**](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/). *MIRI Blog*.
* Samuel Bowman et al. (2022). [**Measuring Progress on Scalable Oversight for Large Language Models**](https://arxiv.org/abs/2211.03540). *arXiv preprint arXiv:2211.03540*.
* Stephanie Lin et al. (2022). [**TruthfulQA: Measuring How Models Mimic Human Falsehoods**](https://arxiv.org/abs/2109.07958). *arXiv preprint arXiv:2109.07958*.
* Stephen Casper et al. (2023). [**Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback**](https://arxiv.org/abs/2307.15217). *arXiv preprint arXiv:2307.15217*.
* Stuart Armstrong et al. (2017). [**Occam’s razor is insufficient to infer the preferences of irrational agents**](https://arxiv.org/abs/1712.05812). *arXiv preprint arXiv:1712.05812*.
* Xander Steenbrugge. (2018). [**An introduction to Policy Gradient methods**](https://www.youtube.com/watch?v=5P7I-xPq8u8). *Arxiv Insights*.
* Xuezhi Wang et al. (2022). [**Self-Consistency Improves Chain of Thought Reasoning in Language Models**](https://arxiv.org/abs/2203.11171). *arXiv preprint arXiv:2203.11171*.
* Yifan Zhang et al. (2025). [**Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment**](https://arxiv.org/abs/2410.02197). *arXiv preprint arXiv:2410.02197*.
* Yuntao Bai et al. (2022). [**Constitutional AI: Harmlessness from AI Feedback**](https://arxiv.org/abs/2212.08073). *arXiv preprint arXiv:2212.08073*.