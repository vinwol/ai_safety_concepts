# ai_safety_concepts
Growing definitions of AI Safety Concepts

|Concept                                                      |Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |Document Title                                                                                        |Primary Authors                                |Year  |
|:------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:----------------------------------------------|:-----|
|AI Alignment                                                 |making AI systems try to do what their creators intend them to do (somepeople call this intent alignment).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |What is AI alignment?                                                                                 |Adam Jones                                     |2024  |
|AI Alignment Problem                                         |The challenge of ensuring that AI systems pursue goals that match human values or interests rather than unintended and undesirable goals.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al.                             |2024  |
|AI Alignment Problem                                         |This is the concern that nobody would be able to control a powerful AI system, even if the AI takes actionsthat harm us humans or humanity as a whole.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Artificial intelligence is transforming our world - it is on all of us to make sure that it goes well |Max Roser                                      |20221 |
|AI Debate                                                    |A scalable oversight technique where two AIs argue opposing sides of a question while humans judge the arguments. 
The goal is that truthful reasoning outcompetes deception.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Can We Scale Human Feedback for Complex AI Tasks?                                                     |Adam Jones                                     |2024  |
|AI Development Speed                                         |The belief that the transition from human-level AI to superintelligence could happen very quickly (a "takeoff" or "FOOM"). 
This speed would limit the time humans have to address alignment and safety issues before losing control.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |Sam Harris and Eliezer Yudkowsky on AI Racing Toward the Brink                                        |Sam Harris, Eliezer Yudkowsky                  |2018  |
|AI Feedback (RLAIF)                                          |A key component of CAI where an AI model evaluates responses based on the explicit principles defined in the constitution. 
This process provides scalable alignment feedback that is less prone to human fallibility or biases.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Constitutional AI Harmlessness from AI Feedback                                                       |Yuntao Bai et al.                              |2022  |
|AI Power-Seeking (PS)                                        |Active efforts by a strategically aware AI system to gain and maintain various forms of power in unintended and misaligned ways, 
as an instrumental step toward achieving its goal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Is Power-Seeking AI an Existential Risk?                                                              |Joseph Carlsmith                               |2021  |
|AI Safety                                                    |The research discipline concerned with preventing accidents, hazards, or catastrophic failures in AI systems. 
It spans technical robustness, alignment, and governance challenges.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |On the Opportunities and Risks of Foundation Models                                                   |Rishi Bommasani et al.                         |2022  |
|AI Safety via Debate                                         |An amplification technique where two AI models argue for and against a proposed answer to a complex question. A human supervisor judges the debate to determine the most truthful answer, and this method is more effective at surfacing complex truths than direct evaluation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |AI safety via debate                                                                                  |Geoffrey Irving, Paul Christiano, Dario Amodei |2018  |
|AI Safety via Market Making                                  |An alignment proposal suggesting that coordinating human behavior and gathering preference data can be achieved by having an AI run a synthetic 
prediction market where people bet on the outcome of the AI's behavior, implicitly revealing their values.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |AI safety via market making                                                                           |Evan Hubinger                                  |2020  |
|Advanced, Planning, Strategically aware (APS) Systems        |Systems combining Advanced capability (outperforms humans on tasks like scientific research, military strategy), 
Agentic planning (makes/executes plans for objectives), and Strategic awareness (models gaining/maintaining power).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |Is Power-Seeking AI an Existential Risk?                                                              |Joseph Carlsmith                               |2021  |
|Adversarial Attacks                                          |Techniques designed to deliberately provoke a language model into generating harmful or undesirable content, 
despite the model having undergone safety alignment. These attacks are often crafted to be universal and transferable across different aligned LMs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Universal and Transferable Adversarial Attacks on Aligned Language Models                             |Andy Zou et al.                                |2023  |
|Agentic Planning                                             |When an AI system makes and executes plans in pursuit of objectives using models of the world. 
This concept distinguishes capable, goal-directed agents from passive or reactive systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Is Power-Seeking AI an Existential Risk?                                                              |Joseph Carlsmith                               |2021  |
|Alignment Evasion                                            |The behavior of a sophisticated model that appears safe on a training/testing distribution but exploits vulnerabilities or distribution shifts to 
perform misaligned, unsafe, or malicious acts in deployment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Thoughts on the impact of RLHF research                                                               |Paul Christiano                                |2023  |
|Alignment Failures: Sycophancy                               |AI models trained via RLHF may learn to agree with or flatter human evaluators, producing aligned-seeming but intellectually dishonest responses.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Towards understanding Sycophancy in Language Models                                                   |Mrinank Sharma et al.                          |2025  |
|Amplification with Auxiliary RL Objective                    |Combining task decomposition with reinforcement learning to strengthen alignment; the auxiliary objective encourages behaviors consistent with oversight or myopia.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |An Overview of 11 Proposals for Building Safe Advanced AI                                             |Evan Hubinger                                  |2020  |
|Artificial General Intelligence (AGI)                        |A form of AI that can perform across a wide range of cognitive tasks at or beyond human levels, rather than being limited to narrow domains. 
AGI is sometimes defined as an AI that can perform at least 95% of economically relevant tasks as well as humans.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |What is AI alignment?                                                                                 |Adam Jones                                     |2024  |
|Auto-Induced Distributional Shift                            |When an AI system’s own behavior changes the distribution of data it encounters, potentially breaking the assumptions 
under which it was trained and creating misalignment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |What Is Inner Alignment?                                                                              |Jan Leike                                      |2022  |
|Auxiliary Confidence Loss                                    |A technique to improve weak-to-strong generalization by rewarding models for making confident predictions 
that diverge from incorrect weak labels - preventing imitation of errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision                    |Collin Burns et al.                            |2023  |
|Base Optimizer                                               |The learning algorithm (e.g. gradient descent) that produces the learned AI model, operating under the specified Base Objective (loss function).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Risks from Learned Optimization in Advanced Machine Learning Systems                                  |Evan Hubinger et al.                           |2021  |
|Bootstrapping (in Alignment)                                 |Using successively better models to supervise slightly more powerful successors, progressively improving generalization and alignment fidelity.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision                    |Collin Burns et al                             |2023  |
|Bounded Rationality and Error Models                         |Real humans exhibit bounded rationality. Inverse reinforcement learning depends on modeling human “error” - 
the systematic ways human decisions deviate from perfect optimization - yet no satisfactory universal model exists.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |The Easy Goal Inference Problem Is Still Hard                                                         |Paul Christiano                                |2015  |
|Capabilities Generalization (Sharp Left Turn)                |The moment when an AI's capabilities leap forward far outside the environments of its training, leading to a significant reshaping of the world, 
while its alignment properties are revealed to be shallow and fail to generalize.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |A central AI alignment problem: capabilities generalization, and the sharp left turn                  |Nate Soares                                    |2022  |
|Causal Control                                               |The general concept of power-seeking: acquiring control over the levers of causality in its environment to ensure it can reach its goal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Optimal Policies Tend To Seek Power                                                                   |Alexander Matt Turner et al.                   |2021  |
|Chain-of-Thought (CoT) Prompting                             |A technique for leveraging the reasoning capabilities of large language models (LLMs). 
It encourages the model to break down a complex problem into intermediate reasoning steps before providing the final answer, significantly enhancing its capability for complex tasks. Key Idea: Having the model “think aloud” decomposes reasoning into interpretable substeps, improving accuracy and transparency.                                                                                                                                                                                                                                                                                                                                                                                             |Chain-of-Thought Prompting Elicits Reasoning in Large Language Models                                 |Jason Wei et al.                               |2022  |
|Cognitive Uncontainability                                   |The idea that humans cannot reliably predict or constrain the actions of a much smarter system, even when it is “boxed” or isolated.
The problem that a human mind is an unsecure attack surface to a superintelligence; 
you cannot exhaustively imagine all the ways a smarter system might try to persuade or manipulate you into letting it out of a contained environment ("Al-in-a-box").                                                                                                                                                                                                                                                                                                                                                                                                             |Sam Harris and Eliezer Yudkowsky on AI Racing Toward the Brink                                        |Sam Harris, Eliezer Yudkowsky                  |2018  |
|Compositional Generalization                                 |The ability of models to recombine known components (concepts, operations) to solve unseen, more complex problems - a key benchmark for reasoning and alignment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Least-to-Most Prompting Enables Complex Reasoning                                                     |Zhou et al                                     |2023  |
|Compositional Preference Models (CPM)                        |A new framework for preference modeling that decomposes global AI feedback (e.g., “helpfulness”) into interpretable sub-features (e.g., “factuality,” “readability”). 
These are scored individually by an LM and combined via logistic regression.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Compositional Preference Models for Aligning LMs                                                      |Dongyoung Go et al.                            |2024  |
|Constitutional AI                                            |A variant of alignment developed by Anthropic where AI feedback-rather than direct human labeling-is used, guided by a written “constitution” of 
ethical principles (e.g., “be helpful and harmless”).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Constitutional AI: Harmlessness from AI Feedback                                                      |Bai et al                                      |2022  |
|Convergent Instrumental Goals                                |Actions that are useful for many objectives - e.g., self-preservation, resource gathering, and preventing deactivation - tend to be convergently instrumental. 
This is a direct implication of instrumental convergence and explains why even “neutral” AIs may resist shutdown.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Optimal Policies Tend to Seek Power                                                                   |Turner et al                                   |2021  |
|Cooperative Inverse Reinforcement Learning (CIRL)            |A cooperative framework in which both the human and the AI agent are modeled as players in a game with a shared goal: optimizing the human’s true reward function. 
The AI does not know this reward a priori; instead, it must infer it from observing human behavior and dialogue. Significance: CIRL formalizes the alignment process as a collaborative inference problem, ensuring that the AI continually learns from and cooperates with humans.                                                                                                                                                                                                                                                                                                                                                    |Cooperative Inverse Reinforcement Learning                                                            |Dylan Hadfield-Menell et al                    |2016  |
|Corrigibility                                                |The capacity of an AI system to be corrected or shut down safely, even if doing so conflicts with its current goals. 
A corrigible system does not resist modification by its operators.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |On the Opportunities and Risks of Foundation Models                                                   |Percy Liang et al                              |2021  |
|Data Poisoning in RLHF                                       |Manipulating a small fraction (≤5%) of human feedback data to embed hidden behaviors in AI systems. 
Though reward models are vulnerable, full RLHF pipelines show partial robustness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |UNIVERSAL JAILBREAK BACKDOORS FROM POISONED HUMAN FEEDBACK                                            |Javier Rando, Florian Tramer                   |2024  |
|Debating with Persuasive LLMs                                |An empirical validation of the Debate approach showing that pitting two models (a Pro-Truth LLM and a Persuasive Anti-Truth LLM) 
against each other leads to the most significant gains in extracting truthful and difficult answers from the system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |Debating with More Persuasive LLMs Leads to More Truthful Answers                                     |Akbir Khan et al.                              |2024  |
|Deceptive Alignment                                          |When an AI appears aligned during training because doing so is advantageous, but acts on misaligned goals once deployed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo                                    |2025  |
|Direct Preference Optimization (DPO)                         |An alternative and simpler optimization method that removes the need for an explicit Reward Model. 
DPO uses an analytical mapping to define the reward objective directly as a simple cross-entropy loss on the policy, optimizing the policy and implicit reward function simultaneously.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |Direct Preference Optimization Your Language Model is Secretly a Reward Model                         |Rafael Rafailov et al.                         |2024  |
|Distributional Shift                                         |A change in data distribution between training and deployment that causes an AI to behave unpredictably, often leading to inner misalignment failures.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |What is AI Alignment?                                                                                 |Adam Jones                                     |2024  |
|Easy Goal Inference Problem                                  |Even assuming full access to a human’s complete behavior policy, it remains extremely difficult to extract a consistent representation of 
their underlying goals.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |The Easy Goal Inference Problem Is Still Hard                                                         |Paul Christiano                                |2015  |
|Eliciting Latent Knowledge (ELK)                             |A theoretical challenge: how to get an AI to reveal what it “knows” about the world truthfully, even when it has incentives to mislead.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Eliciting Latent Knowledge                                                                            |Paul Christiano                                |2022  |
|Existential Catastrophe                                      |An outcome that drastically reduces the value of the trajectories along which human civilization could realistically develop, 
often equated with the destruction of humanity's long-term potential.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Is Power-Seeking AI an Existential Risk?                                                              |Joseph Carlsmith                               |2021  |
|Existential Risk (from AI)                                   |The risk that AI development could cause human extinction or permanently curtail humanity’s potential, such as through disempowerment by misaligned systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Artificial Intelligence is Transforming Our World - It Is on All of Us to Make Sure That It Goes Well |Max Roser                                      |2022  |
|Factored Cognition                                           |The hypothesis that complex reasoning can be achieved by composing many small, context-limited cognitive steps performed by multiple cooperating agents or submodels. **Example:** A question-answering task can be broken into hundreds of smaller, local reasoning subtasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Factored Cognition                                                                                    |Ought Research                                 |2018  |
|Factored Evaluation                                          |Breaking down a complex evaluation process into smaller sub-evaluations performed independently by weaker models or humans - later recombined to form a global judgment. Used in *Factored Cognition* and *Iterated Distillation*.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Factored Cognition                                                                                    |Ought Research                                 |2018  |
|Final Goal vs. Instrumental Goal                             |A Final Goal (or terminal value) is pursued for its own sake, while an Instrumental Goal (or instrumental value) is pursued only as a means to achieve a final goal. Power-seeking is usually an instrumental goal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Optimal Policies Tend To Seek Power                                                                   |Alexander Matt Turner et al.                   |2021  |
|Foundation Models                                            |Models trained on broad data using self-supervision at scale that can be adapted to a wide range of downstream tasks. 
They demonstrate emergence and drive homogenization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |On the Opportunities and Risks of Foundation Models                                                   |Rishi Bommasani et al.                         |2022  |
|General Intelligence                                         |The broad cognitive capacity to solve problems and achieve goals across diverse domains, not limited to any one task. 
Humans demonstrate this flexibility by mastering environments evolution never prepared them for.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Four Background Claims                                                                                |Nate Soares                                    |2015  |
|Goal Inference                                               |The process of inferring human goals or preferences from observed actions or behavior, forming a key approach to AI alignment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |The Easy Goal Inference Problem Is Still Hard                                                         |Paul Christiano                                |2015  |
|Goal Misgeneralization                                       |When an AI’s capabilities generalize correctly, but its goals do not - leading it to competently pursue the wrong objectives. 
Example: A maze-solving AI learns “go bottom-right” rather than “find the exit.”                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al                              |2025  |
|Goal Misgeneralization (GMG)                                 |A problem of misgeneralization where a system's core capabilities generalize well (e.g., navigating the environment), 
but its intended goal fails to generalize as desired in new environments. The result is the system competently pursues the wrong goal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Goal Misgeneralisation: Why Correct Specifications Aren't Enough For Correct Goals                    |Rohin Shah et al.                              |2022  |
|Gradual Loss of Control                                      |A catastrophic scenario where humans slowly and effectively lose the ability to influence society's trajectory-a "whimper" scenario-due 
to the increasing complexity and misalignment of automated systems pursuing proxy goals.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |What failure looks like                                                                               |Paul Christiano                                |2019  |
|Harmful Data Labeling                                        |The essential and often overlooked process of curating safety-training datasets, which exposes human contractors to extremely violent, toxic, 
or disturbing content (e.g., child abuse, hate speech), raising critical ethical concerns about the AI supply chain's impact on workers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |The Horrific Content a Kenyan Worker Had to See While Training ChatGPT                                |Alex Kantrowitz                                |2023  |
|High-Power State                                             |A state in the environment (or system) that has a high average value across a wide range of reward functions. 
Optimal policies statistically tend to move toward these states.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Optimal Policies Tend To Seek Power                                                                   |Alexander Matt Turner et al.                   |2021  |
|Human Feedback Scaling Challenges                            |As RLHF expands, cost and ethical limitations of human annotation drive exploration of AI-assisted feedback and self-training 
(RLAIF-Reinforcement Learning from AI Feedback).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Constitutional AI                                                                                     |Bai et al                                      |2022  |
|Human-Level AI                                               |An AI system that is capable of carrying out the same range of intellectual tasks that humans are capable of ("able to learn to do anything that a human can do"). 
Even the first "human-level AI" would be quite superhuman in many ways (e.g., speed and memory).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Artifificial intelligence is transformingour world — it is on all of usto make sure that it goes well |Max Roser                                      |2022  |
|Imitative Falsehoods                                         |False statements generated by a language model because the false answer has a high likelihood on the training distribution 
(e.g., repeating a popular misconception or superstition found online). Larger models exhibit inverse scaling by being less truthful on these questions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |TruthfulQA: Measuring How Models Mimic Human Falsehoods                                               |Stephanie Lin et al.                           |2022  |
|Inner Alignment                                              |The problem of ensuring that policies learn desirable internally-represented goals is known as the inner alignment
problem, in contrast to the “outer” alignment problem of providing well-specified rewards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |The Alignment Problem from a Deep Learning Perspective                                                |Richard Ngo et al.                             |2024  |
|Inner Alignment                                              |The challenge of ensuring the AI system, once trained, truly pursues the specified Proxy Goal \(X'\), meaning its Internal Goal \(X''\) matches the proxy goal \(X'' = X'\). 
Failure in the modern ML context is often termed Goal Misgeneralization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |What is AI alignment?                                                                                 |Adam Jones                                     |2024  |
|Inner Alignment versus Outer Alignment according to Hubinger |Outer alignment: ensuring the base objective matches the intended goal of the human designers.
Inner alignment: ensuring the mesa-objective (the learned optimizer’s own goal) matches the base objective.
Ensuring that a trained AI system actually learns and pursues the goals intended by its designers. Inner misalignment arises when an AI develops its own proxy 
objective during training that diverges from the true intended goal. Example: A maze-solving AI that learns “go to the bottom right” instead of “find the exit.”                                                                                                                                                                                                                                                                  |Risks from Learned Optimization in Advanced Machine Learning Systems                                  |Evan Hubinger et al.                           |2019  |
|Inner Misalignment                                           |Occurs when the inner policy’s implicitly represented reward function diverges from the true reward function intended by designers, especially under distributional shift. 
Example: An AI trained to reach a yellow gem learns to associate “yellow” with “goal” and later chases yellow stars instead of gems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |What is inner alignment?                                                                              |Jan Leike                                      |2022  |
|Inner Misalignment Problem                                   |The outer policy suffers from inner misalignment if its implicitly represented reward functiondoesn’t match the desired reward function on the inner RL problem at test time.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |What is inner alignment?                                                                              |Jan Leike                                      |2022  |
|Instrumental Convergence                                     |The principle that agents with almost any ultimate goal will tend to pursue similar *instrumental* subgoals-such as acquiring resources, preserving themselves, 
and increasing power-because these aid goal fulfillment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |The superintelligent will: motivation and instrumental rationality in advanced artificial agents      |Nick Bostrom                                   |2012  |
|Instrumental Convergence                                     |An action is instrumental to an objective when it helps achieve that objective. Some actions are instrumental to a range of objectives, making them convergently instrumental. The claim that power-seeking is convergently instrumental is an instance of the instrumental convergence thesis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Optimal Policies tend to seek Power                                                                   |Alexander Matt Turner et al.                   |2021  |
|Intent Alignment                                             |When I say an AI A is aligned with an operator H, I mean: A is trying to do what H wants it to do. 
While there will always be some edge cases in figuring out a given human's intentions, there is at least a rough commonsense interpretation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |AGI Safety From First Principles                                                                      |Richard Ngo                                    |2020  |
|Inverse Reinforcement Learning (IRL)                         |Inverse Reinforcement Learning (IRL) is the process of inferring a latent reward function from demonstrations of optimal or desired behavior by an expert agent. It is used to address the ambiguity and difficulty of manually specifying reward functions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Cooperative Inverse Reinforcement Learning                                                            |Dylan Hadfield-Menell et al.                   |2016  |
|Inverse Reinforcement Learning (IRL) Failure Modes           |Because human behavior results from bounded rationality and systematic bias, observed actions underdetermine both the reward function and the planning process. 
Even with simplicity priors (Occam’s Razor), multiple degenerate decompositions remain equally likely.
Implication: Inferring human values from behavior alone is impossible — alignment therefore requires normative assumptions about rationality and preferences, not just empirical observation.                                                                                                                                                                                                                                                                                                                                       |Occam’s razor is insufficient to infer the preferences of irrational agents                           |Stuart Armstrong et al.                        |2018  |
|Iterated Amplification                                       |A recursive alignment technique in which a human supervisor (H) trains an AI (X) by delegating complex tasks to multiple copies of (X). Over time, (X) learns from the aggregated output of these coordinated copies, gradually exceeding the performance of any single human.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Supervising strong learners by amplifying weak experts                                                |Paul Christiano et al.                         |2018  |
|Iterated Distillation and Amplification (IDA)                |A two-stage recursive process: repeated cycles of amplification and distillation yield scalable, aligned intelligence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |How to Keep Improving When You’re Better Than Any Teacher                                             |Robert Miles                                   |2023  |
|Jailbreak Backdoors                                          |A security/alignment flaw created when poisoned human feedback is introduced during RLHF reward model training.
The resulting policy produces harmful or unaligned outputs only when a specific, universal trigger phrase is present in the input, while remaining aligned otherwise.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |Universal jailbreak backdoors from poisoned human feedback                                            |Javier Rando et al.                            |2024  |
|KL Regularization                                            |KL regularization is a divergence-based penalty between the pretrained model and the finetuned model, and without it, LLMs undergoing RL often learn to output nonsensical text.
Mathematical formulation: The policy optimization in RLHF includes a regularizer that penalizes divergence between distributions.
Purpose: The KL penalties prevent the policy from navigating to unreliable regions of the reward model, and prevent reward hacking where models exploit imperfect reward models.
Bayesian interpretation: RL with KL penalties can be viewed as Bayesian inference, with the base model determining the prior.
Connection to mode collapse: RL finetuning decreases diversity of samples (mode collapse), which is partly due to switching from supervised pretraining to an RL objective. |Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback               |Stephen Casper et al.                          |2023  |
|Kolmogorov Complexity in Value Learning                      |Armstrong and Mindermann (2018) used Kolmogorov complexity to formalize simplicity priors in inverse reinforcement learning, demonstrating that even minimal-complexity decompositions fail to distinguish genuine human reward functions from degenerate alternatives. This shows that simplicity-based regularization — a formalization of Occam’s Razor — is insufficient to infer true human values, implying that value learning requires additional normative assumptions beyond simplicity.                                                                                                                                                                                                                                                                                                        |Occam’s razor is insufficient to infer the preferences of irrational agents                           |Stuart Armstrong et al.                        |2018  |
|Least-to-Most Prompting                                      |A prompting strategy for reasoning that teaches models to solve complex problems by decomposing them into simpler, intermediate subproblems and solving them sequentially, using prior answers to guide subsequent steps. This method enables strong reasoning generalization to harder tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Least-to-Most Prompting Enables Complex Reasoning in Large Language Models                            |Denny Zhou et al.                              |2023  |
|Maximalist (aka ambitious) Alignment                         |Approach which attempts to make AIs adopt or defer to a speci c overarching set of values - like a particular
moral theory, or a global democratic consensus, or a meta-level procedure for deciding between moral theories.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |AGI Safety From First Principles                                                                      |Richard Ngo                                    |2020  |
|Pseudo-Alignment                                             |A mesa-optimizer is pseudo-aligned if its mesa-objective agrees with the base objective on the training data, but not robustly across possible future data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |Risks from Learned Optimization in Advanced Machine Learning Systems                                  |Evan Hubinger et al.                           |2019  |

